# -*- coding: utf-8 -*-
"""
æ—¥æœ¬èªãƒã‚¹ã‚¿ãƒ¼ï¼ˆæœˆæ¬¡CSV: race_master_jp_YYYYMM.csvï¼‰ã‚’èª­ã¿è¾¼ã¿ã€
è‹±èªæ•´å½¢ã—ã¦å˜ä¸€ã® master ãƒ†ãƒ¼ãƒ–ãƒ« (data/raw/race_master.csv) ã‚’å‡ºåŠ›ã€‚

æ—¢å®šã¯ 2024å¹´ 1..12 ã‚’å¯¾è±¡ã€‚
å°†æ¥ã¯ --year-from 2022 --year-to 2024 ã®ã‚ˆã†ã« 3å¹´åˆ†ã‚‚ä¸€æ‹¬å‡¦ç†å¯èƒ½ã€‚

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  --months 1,2,3     æœˆã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚ŠæŒ‡å®šï¼ˆæœªæŒ‡å®šãªã‚‰ 1..12ï¼‰
  --strict           2/3æ­³ãƒ»1/2å‹ãƒ»èŠãƒ»è·é›¢1000ã€œ1700 ã®å³å¯†ãƒ•ã‚£ãƒ«ã‚¿ã‚’æœ€çµ‚æ®µã§é©ç”¨
"""

from __future__ import annotations
import argparse
import glob
from pathlib import Path
from typing import List, Optional

import pandas as pd

RAW_DIR  = Path("data/raw")
OUT_CSV  = RAW_DIR / "race_master.csv"

VENUE_MAP = {
    "æœ­å¹Œ": "SAPPORO","å‡½é¤¨": "HAKODATE","ç¦å³¶": "FUKUSHIMA","æ–°æ½Ÿ": "NIIGATA",
    "æ±äº¬": "TOKYO","ä¸­å±±": "NAKAYAMA","ä¸­äº¬": "CHUKYO","äº¬éƒ½": "KYOTO",
    "é˜ªç¥": "HANSHIN","å°å€‰": "KOKURA",
}
SURFACE_MAP = {"èŠ": "TURF", "ãƒ€": "DIRT", "ãƒ€ãƒ¼ãƒˆ": "DIRT"}  # å¿µã®ãŸã‚ "ãƒ€" ã‚‚å¸å
TRACK_COND_MAP = {"è‰¯": "Gd", "ç¨é‡": "Yld", "é‡": "Sft", "ä¸è‰¯": "Hvy"}
WEATHER_MAP = {
    "æ™´": "Sunny", "æ™´ã‚Œ": "Sunny",
    "æ›‡": "Cloudy", "æ›‡ã‚Š": "Cloudy",
    "å°é›¨": "LightRain", "é›¨": "Rain",
    "å°é›ª": "LightSnow", "é›ª": "Snow",
}

# å‡ºåŠ›åˆ—ï¼ˆè‹±èªãƒ“ãƒ¥ãƒ¼ã®æœ€å°ã‚»ãƒƒãƒˆï¼‰
EN_COLS = ["race_id","date","venue","surface_type","distance","weather","track_condition","num_horses","url_jp"]

def _parse_months_arg(arg: Optional[str]) -> Optional[List[int]]:
    if not arg:
        return None
    parts = []
    for tok in arg.split(","):
        tok = tok.strip()
        if not tok:
            continue
        if "-" in tok:
            a, b = tok.split("-", 1)
            a, b = int(a), int(b)
            parts.extend(range(min(a,b), max(a,b)+1))
        else:
            parts.append(int(tok))
    # é‡è¤‡ã‚„é †åºã¯æ°—ã«ã—ãªã„ï¼ˆå¾Œæ®µã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ä¸è¦ï¼‰
    return parts

def _load_monthlies(year_from: int, year_to: int, months: Optional[List[int]]) -> pd.DataFrame:
    paths = []
    if months is None:
        months = list(range(1, 13))

    for y in range(year_from, year_to + 1):
        for m in months:
            pat = RAW_DIR / f"race_master_jp_{y}{m:02d}.csv"
            if pat.exists() and pat.stat().st_size > 0:
                paths.append(str(pat))

    if not paths:
        print("[WARN] No monthly JP files were found. Pattern: data/raw/race_master_jp_YYYYMM.csv")
        return pd.DataFrame()

    frames = []
    for p in sorted(paths):
        try:
            df = pd.read_csv(p, dtype={"race_id": str})
            frames.append(df)
            print(f"[READ] {p} rows={len(df)}")
        except Exception as e:
            print(f"[WARN] failed to read {p}: {e}")

    if not frames:
        return pd.DataFrame()

    out = pd.concat(frames, ignore_index=True)
    # ä¸»ã‚­ãƒ¼é‡è¤‡ã¯æœ€åˆã‚’å„ªå…ˆï¼ˆåŒä¸€IDã®æœˆé‡è¤‡ã¯åŸºæœ¬èµ·ããªã„æƒ³å®šã ãŒå¿µã®ãŸã‚ï¼‰
    out = out.drop_duplicates("race_id")
    return out

def translate_master(df: pd.DataFrame) -> pd.DataFrame:
    """JPâ†’EN æ­£è¦åŒ–ï¼ˆåˆ—åã‚†ã‚‰ãã«ã‚‚å¯¾å¿œï¼‰ã€‚"""
    if df.empty:
        return df.copy()

    df = df.copy()

    # åˆ—åã‚†ã‚‰ãå¸å
    rename_map = {
        "å ´æ‰€": "venue",
        "venue_jp": "venue",
        "surface_jp": "surface_type",
        "track_condition_jp": "track_condition",
        "track_condision_jp": "track_condition",  # ã‚¿ã‚¤ãƒå¯¾ç­–
        "å¤©å€™": "weather",
        "weather_jp": "weather",
    }
    df.rename(columns=rename_map, inplace=True)

    # venue
    if "venue" in df.columns:
        df["venue"] = df["venue"].map(VENUE_MAP).fillna(df["venue"])

    # surface_type
    if "surface_type" in df.columns:
        df["surface_type"] = df["surface_type"].map(SURFACE_MAP).fillna(df["surface_type"])

    # track_condition
    if "track_condition" in df.columns:
        df["track_condition"] = df["track_condition"].map(TRACK_COND_MAP).fillna(df["track_condition"])

    # weather
    if "weather" in df.columns:
        df["weather"] = df["weather"].replace(WEATHER_MAP)

    # num_horses ãŒç„¡ã„å ´åˆã¯ä½œã£ã¦ãŠãï¼ˆå‰æ®µã§çµåˆã—ãªã„æ§‹æˆã§ã‚‚å£Šã‚Œãªã„ã‚ˆã†ã«ï¼‰
    if "num_horses" not in df.columns:
        df["num_horses"] = pd.NA

    # URL åˆ—ã¯åç§°ãŒé•ã†ã‚±ãƒ¼ã‚¹ã‚’å¸å
    if "url_jp" not in df.columns:
        for cand in ["jp_url", "race_url", "race_url_jp", "url"]:
            if cand in df.columns:
                df.rename(columns={cand: "url_jp"}, inplace=True)
                break
    if "url_jp" not in df.columns:
        df["url_jp"] = pd.NA

    # å‡ºåŠ›åˆ—ã‚’å›ºå®šé †ã«
    for c in EN_COLS:
        if c not in df.columns:
            df[c] = pd.NA
    return df[EN_COLS].copy()

def _apply_strict_filter_if_requested(df: pd.DataFrame, strict: bool) -> pd.DataFrame:
    """å¿…è¦ãªã‚‰æœ€çµ‚æ®µã§å³å¯†ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ2/3æ­³ãƒ»1/2å‹ãƒ»èŠãƒ»è·é›¢ 1000-1700ï¼‰ã€‚"""
    if not strict or df.empty:
        return df

    # å‰æ®µã® JP ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¾å­˜ã™ã‚‹åˆ—ï¼ˆage_exact / race_class_jp / surface_jp / distanceï¼‰ãŒ
    # å¤±ã‚ã‚Œã¦ã„ã‚‹å ´åˆã«å‚™ãˆã€å…ƒã®åˆ—åãŒæ®‹ã£ã¦ã„ã‚Œã°ãã‚Œã‚’å‚ç…§ã™ã‚‹ã€‚
    # translate å‰ã®åˆ—ã‚’ä¿æŒã—ã¦ã„ã‚‹æƒ³å®šã§ã‚ã‚Œã°ã€ã“ã“ã§å†ãƒ•ã‚£ãƒ«ã‚¿å¯ã€‚
    # ãªã‘ã‚Œã° EN ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰åˆ¤å®šå¯èƒ½ãªæ¡ä»¶ï¼ˆsurface_type / distanceï¼‰ã ã‘é©ç”¨ã€‚
    have_jp_filter_cols = all(c in df.columns for c in ["surface_type", "distance"])
    if not have_jp_filter_cols:
        return df

    mask = (df["surface_type"] == "TURF") & df["distance"].between(1000, 1700, inclusive="both")
    # age / class ã¯ EN ãƒ“ãƒ¥ãƒ¼ã«ã¯ç„¡ã„ã®ã§ã€JPåˆ—ãŒæ®‹ã£ã¦ã„ã‚Œã°ã•ã‚‰ã«æ›ã‘ã‚‹
    age_col, class_col = None, None
    for c in ["age_exact", "age", "age_jp"]:
        if c in df.columns:
            age_col = c; break
    for c in ["race_class_jp", "race_class"]:
        if c in df.columns:
            class_col = c; break

    if age_col is not None:
        mask &= df[age_col].isin([2, 3])
    if class_col is not None:
        mask &= df[class_col].astype(str).isin(["1å‹","2å‹","1WIN","2WIN"])

    filtered = df[mask].copy()
    filtered = filtered.drop_duplicates("race_id")
    print(f"[STRICT] filtered {len(df)} -> {len(filtered)} rows")
    return filtered

def main():
    parser = argparse.ArgumentParser(description="Translate JP monthly masters to single EN master CSV.")
    parser.add_argument("--year-from", type=int, default=2024)
    parser.add_argument("--year-to", type=int, default=2024)
    parser.add_argument("--months", type=str, default=None, help="ä¾‹: '1-12' or '1,2,3'")
    parser.add_argument("--strict", action="store_true", help="2/3æ­³ãƒ»1/2å‹ãƒ»èŠãƒ»è·é›¢1000-1700ã®å³å¯†ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨")
    parser.add_argument("--out", type=str, default=str(OUT_CSV))
    args = parser.parse_args()

    months = _parse_months_arg(args.months)
    df_jp = _load_monthlies(args.year_from, args.year_to, months)

    if df_jp.empty:
        print("[WARN] No input rows. Nothing to write.")
        return

    print("ğŸ“„ èª­ã¿è¾¼ã‚“ã åˆ—ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰:", df_jp.columns.tolist()[:25])
    df_en = translate_master(df_jp)
    df_en = _apply_strict_filter_if_requested(df_en, args.strict)

    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    df_en.to_csv(out_path, index=False)
    print(f"âœ… wrote {out_path}, rows={len(df_en)}")

if __name__ == "__main__":
    main()